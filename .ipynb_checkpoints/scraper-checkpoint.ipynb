{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "#import ast\n",
    "from all_functions import *\n",
    "import math\n",
    "import requests\n",
    "from pylatex import Document, Section, Subsection, Command\n",
    "from pylatex.utils import italic, NoEscape\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import codecs\n",
    "import unidecode\n",
    "import datasheets\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address(query, api_key):\n",
    "        base = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?input='\n",
    "        locationbias= '42.3314584645106, -71.1039191294055'\n",
    "        #key = api_key\n",
    "        query = query\n",
    "        fields = 'formatted_address,name,geometry'\n",
    "        inputtype = 'textquery'\n",
    "        \n",
    "        url_search = base + query + '&inputtype=' + inputtype + '&fields=' + fields + '&locationbias=circle:2000@' + locationbias + '&key=' + api_key\n",
    "        print(url_search)\n",
    "        my_dict = requests.get(url_search).json()\n",
    "        return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commute(dest, origin, api_key):\n",
    "    \n",
    "    base = 'https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&'\n",
    "    origin = 'origins=' + origin + '&'\n",
    "    dest_str = 'destinations=' + dest + '&'\n",
    "    api_key = 'key=' + api_key\n",
    "    \n",
    "    url_search = base + origin + dest_str + api_key\n",
    "    print(url_search)\n",
    "    my_dict = requests.get(url_search).json()\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_duration(company_list, id_list, api_key):\n",
    "    comm_df = pd.DataFrame(columns=['company', 'place', 'address', 'distance', 'duration'])\n",
    "    for index, company in enumerate(company_list):\n",
    "        comm_dict = {}\n",
    "        company = company\n",
    "        tmp_id = id_list[int(index)]\n",
    "        text_dict = get_address(str(company), api_key=api_key)\n",
    "        #print(text_dict)\n",
    "        if text_dict['status'] == 'ZERO_RESULTS':\n",
    "            print(company + ' has 0 results in get_address.')\n",
    "            base = 'https://maps.googleapis.com/maps/api/staticmap?'\n",
    "            place = 'None'\n",
    "            address = 'None'\n",
    "            distance = 'None'\n",
    "            duration = 'None'\n",
    "            #params = {'center' : '8 Oswald Street, Boston MA 02120' , 'zoom' : '20', 'size' : '200x200' , 'maptype' : 'roadmap', 'key': api_key}\n",
    "            params = {'center' : '8 Oswald Street, Boston MA 02120' , 'zoom' : '10', 'size' : '200x200' , 'maptype' : 'roadmap', 'key': api_key}\n",
    "            map_url = requests.get(base, params).url\n",
    "        else:\n",
    "            address = text_dict['candidates'][0]['formatted_address']\n",
    "            place = text_dict['candidates'][0]['name']    \n",
    "            res_lat_lng = str(text_dict['candidates'][0]['geometry']['location']['lat']) + ',' + str(text_dict['candidates'][0]['geometry']['location']['lng'])\n",
    "            base = 'https://maps.googleapis.com/maps/api/staticmap?'\n",
    "            #params = {'center' : '8 Oswald Street, Boston MA 02120' , 'zoom' : '18', 'size' : '200x200' , 'maptype' : 'roadmap', 'markers': 'color:blue|label:S|' + res_lat_lng, 'key': api_key} \n",
    "            params = {'center' : '8 Oswald Street, Boston MA 02120' , 'size' : '200x200' , 'maptype' : 'roadmap', 'markers': 'color:blue|label:S|' + res_lat_lng, 'key': api_key} \n",
    "            map_url = requests.get(base, params).url\n",
    "            \n",
    "            \n",
    "            comm = get_commute(dest=res_lat_lng, origin='8 Oswald Street, 02120', api_key=api_key)\n",
    "            if comm['rows'][0]['elements'][0]['status'] == 'ZERO_RESULTS':\n",
    "                print(company + ' has 0 results in get_commute.')\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    distance = (comm['rows'][0]['elements'][0]['distance']['text'])\n",
    "                    duration = (comm['rows'][0]['elements'][0]['duration']['text'])\n",
    "                except:\n",
    "                    #print(comm)\n",
    "                    break\n",
    "\n",
    "        comm_dict['company'] = company\n",
    "        comm_dict['place'] = place\n",
    "        comm_dict['address'] = address\n",
    "        comm_dict['distance'] = distance\n",
    "        comm_dict['duration'] = duration\n",
    "        comm_dict['ID'] = tmp_id\n",
    "        comm_dict['map_url'] = map_url\n",
    "        comm_tmp_df = pd.DataFrame(comm_dict, index = [comm_dict['ID']])\n",
    "        comm_df = comm_df.append(comm_tmp_df, sort=False)\n",
    "    return comm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_scraper(wd, results, job_title, city):\n",
    "    \n",
    "    res_left = results\n",
    "    tot_pg = ((results // 25) + 1)\n",
    "    print('stack total pages: ' + str(tot_pg))\n",
    "    #print('indeed total pages: ' + str(tot_pg))\n",
    "    ID = [str('s-ID-' + str(n)) for n in range(0, results)]\n",
    "    pulldates = [str(time.strftime(\"%m%d%Y\", time.localtime(time.time()))) for n in range(0, results)]\n",
    "    stack_df = pd.DataFrame({'ID' : ID, 'pulldates' : pulldates}).set_index('ID')\n",
    "    for i in range(tot_pg):\n",
    "        url = 'https://stackoverflow.com/jobs?q=python&sort=p&l=boston&d=20&u=Miles&start=' + str(i*25)\n",
    "        pg_IDs = ID[i*25:(i+1)*25]\n",
    "        post_links = [title.get_attribute('href') for title in wd.find_elements_by_xpath(\"//h2[@class='fs-subheading job-details__spaced mb4']//a\")]\n",
    "        #url = 'https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25'\n",
    "        #print(url)\n",
    "        wd.get(url)\n",
    "        #################### search results ####################     \n",
    "        #comp_locs = [comp_loc.text for comp_loc in wd.find_elements_by_xpath(\"//div[@class='-job-summary']/div[contains(@class, '-company')]\")]\n",
    "        stack_df.loc[pg_IDs, 'comp_locs'] = [comp_loc.text for comp_loc in wd.find_elements_by_xpath(\"//div[@class='-job-summary']/div[contains(@class, '-company')]\")]\n",
    "        test_len = len([comp_loc.text for comp_loc in wd.find_elements_by_xpath(\"//div[@class='-job-summary']/div[contains(@class, '-company')]\")])\n",
    "        \n",
    "        res_len = min(test_len, int(res_left))\n",
    "\n",
    "        #l_companies = [company.text.lower().split(',')[0] for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")]\n",
    "        stack_df.loc[pg_IDs, 'titles'] = [title.text for title in wd.find_elements_by_xpath(\"//h2[@class='fs-subheading job-details__spaced mb4']\")]\n",
    "        stack_df.loc[pg_IDs, 'comp_links'] = ['https://www.stackoverflow.com/jobs/company/' + company.text.replace(' ', '-') for company in wd.find_elements_by_xpath(\"//div[@class='fc-black-700 fs-body2 -company']\")]\n",
    "        stack_df.loc[pg_IDs, 'tags'] = [tag.text for tag in wd.find_elements_by_xpath(\"//div[@class='mt12 -tags']\")]\n",
    "        #stack_df.loc[pg_IDs, 'perks'] = [perk.text for perk in wd.find_elements_by_xpath(\"//div[@class='mt2 -perks']\")]\n",
    "        stack_df.loc[pg_IDs, 'postdates'] = [date.text for date in wd.find_elements_by_xpath(\"//span[contains(@class, 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12')]\")]\n",
    "        #description = [description.text for description in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='summary']\")]\n",
    "        stack_df.loc[pg_IDs, 'post_links'] = [title.get_attribute('href') for title in wd.find_elements_by_xpath(\"//h2[@class='fs-subheading job-details__spaced mb4']//a\")]\n",
    "\n",
    "        #comp_links = ['https://www.indeed.com/cmp/' + str(company).replace(' ', '-') for company in companies]                    \n",
    "\n",
    "        '''        if res_len < 25:\n",
    "            comp_locs = comp_locs[:res_len]\n",
    "            titles = titles[:res_len]\n",
    "            tags = tags[:res_len]\n",
    "            perks = perks[:res_len]\n",
    "            post_links = post_links[:res_len]\n",
    "            postdates = postdates[:res_len]\n",
    "            comp_links = comp_links[:res_len]'''\n",
    "            \n",
    "            \n",
    "        #ID = [str('s-ID-' + str(n)) for n in range(results-res_left, results-res_left+len(comp_locs))]\n",
    "        #comp_loc = ['' for n in range(results-res_left, results-res_left+len(comp_locs))]\n",
    "        #companies = [company.split(' - ')[0] for company in comp_locs]\n",
    "        #locations = [location.split(' - ')[1] for location in comp_locs]\n",
    "                    \n",
    "        res_left = res_left - res_len\n",
    "\n",
    "        #print(len(titles), len(companies), len(post_links), len(postdates), len(pulldates), len(comp_links))\n",
    "        #return 'bitch'\n",
    "        #stack_df = pd.DataFrame({'ID' : ID, 'title' : titles, 'company' : companies, 'post_link' : post_links, 'comp_link' : comp_links, 'postdate' : postdates, 'pulldate' : pulldates}).set_index('ID') # functional\n",
    "\n",
    "        \n",
    "        #################### post list ####################     \n",
    "        post_list = []\n",
    "        for index, link in enumerate(post_links):\n",
    "            post_id = ID[index]\n",
    "            wd.get(link)\n",
    "            #average_comp_rating = wd.find_element_by_xpath(\"//span[@class='cmp-header-rating-average']\").text\n",
    "            stack_df.loc[post_id, 'body'] = wd.find_element_by_xpath(\"//div[@id='overview-items']\").text\n",
    "            try:\n",
    "                stack_df.loc[post_id, 'benefits'] = wd.find_element_by_xpath(\"//section[@class='-benefits mb32']\").text\n",
    "            except:\n",
    "                stack_df.loc[post_id, 'benefits'] = ''\n",
    "                #print(link)\n",
    "            res_len = res_len - len(post_links)    \n",
    "            \n",
    "            #post_list.append([str(index), body, benefits])\n",
    "        #comp_df = pd.DataFrame(post_list, columns=['ID', 'body_text', 'benefits']).set_index('ID')\n",
    "        #stack_df = stack_df.merge(comp_df)\n",
    "    return stack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(wd, results, job_title, city, linkedin_username, linkedin_password):\n",
    "    tot_pg = ((results // 25) + 1)\n",
    "    res_left = results\n",
    "    print('linkedin total pages: ' + str(tot_pg))\n",
    "    login_url = 'https://www.linkedin.com/'\n",
    "    wd.get(login_url)\n",
    "    #wd.find_element_by_xpath(\"//a[@class='sign-in-link']\").click\n",
    "    #wd.find_element_by_xpath(\"//a[@class='form-toggle']\").click\n",
    "    login_email = wd.find_element_by_id(\"login-email\")\n",
    "    login_email.send_keys(linkedin_username)\n",
    "    login_pass = wd.find_element_by_id(\"login-password\")\n",
    "    login_pass.send_keys(linkedin_password)\n",
    "    wd.find_element_by_id(\"login-submit\").click\n",
    "    #cookies = wd.get_cookie('bcookie')\n",
    "    \n",
    "    ID = [('L-ID-' + str(n)) for n in range(0, results)]\n",
    "    pulldates = [str(time.strftime(\"%m%d%Y\", time.localtime(time.time()))) for n in range(0, results)]\n",
    "    linkedin_df = pd.DataFrame({'ID' : ID, 'pulldates' : pulldates}).set_index('ID')\n",
    "    for i in range(tot_pg):\n",
    "        url = 'http://www.linkedin.com/jobs/search/?keywords=' + job_title + '&location=' + city + '&sortBy=DD&start=' + str(i*25)\n",
    "        print('Page {} url : {}'.format(i, url))\n",
    "        wd.get(url)\n",
    "        pg_IDs = ID[i*25:(i+1)*25] # Gets 25 IDs relevant to page\n",
    "        res_len = min(len(wd.find_elements_by_xpath(\"//li[@class='job-listing']\")), int(res_left))\n",
    "        companies = [c.text for c in wd.find_elements_by_xpath(\"//span[@class='company-name-text']\")]\n",
    "        locations = [L.text for L in wd.find_elements_by_xpath(\"//span[@class='job-location']/span\")]\n",
    "        comp_links = []\n",
    "        for elem in wd.find_elements_by_xpath(\"//div[@class='company-name']\"):\n",
    "            if elem.find_element_by_xpath(\"//a\") == None:\n",
    "                comp_links.append('NA')\n",
    "            else:\n",
    "                comp_links.append(elem.find_element_by_xpath(\"//a\").get_attribute('href'))\n",
    "        linkedin_df.loc[pg_IDs, 'comp_links'] = comp_links\n",
    "        linkedin_df.loc[pg_IDs, 'post_links'] = [l.get_attribute('href') for l in wd.find_elements_by_xpath(\"//a[@class='job-title-link']\")]\n",
    "        linkedin_df.loc[pg_IDs, 'titles'] = [t.text for t in wd.find_elements_by_xpath(\"//span[@class='job-title-text']\")]\n",
    "        linkedin_df.loc[pg_IDs, 'locations'] = [L.text for L in wd.find_elements_by_xpath(\"//span[@class='job-location']/span\")]\n",
    "        linkedin_df.loc[pg_IDs, 'companies'] = [c.text for c in wd.find_elements_by_xpath(\"//span[@class='company-name-text']\")]\n",
    "        linkedin_df.loc[pg_IDs, 'descr'] = [d.text for d in wd.find_elements_by_xpath(\"//div[@class='job-description']\")]\n",
    "        linkedin_df.loc[pg_IDs, 'ezapply'] = [e.text for e in wd.find_elements_by_xpath(\"//div[@class='job-flavor-in-apply-container']\")]\n",
    "        linkedin_df.loc[pg_IDs, 'company_pics'] = [i.get_attribute('src') for i in wd.find_elements_by_xpath(\"//a[@class='company-logo-link']//img\")]\n",
    "\n",
    "        comp_loc = []\n",
    "        for i, elem in enumerate(companies):\n",
    "            comp_loc_str = (elem.strip().split(',')[1:] + ', ' + locations[i].strip())\n",
    "            comp_loc.append(comp_loc_str)\n",
    "        linkedin_df.loc[pg_IDs, 'comp_loc'] = comp_loc\n",
    "        res_left = res_left - res_len\n",
    "    return linkedin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_scraper(wd, results, job_title, city):\n",
    "    list_o_divs = []\n",
    "    #tot_pg = ((results // 10) + 1)\n",
    "    res_left = results\n",
    "    tot_pg = ((results // 10) + 1)\n",
    "    print('indeed total pages: ' + str(tot_pg))\n",
    "    #print('indeed total pages: ' + str(tot_pg))\n",
    "    ID = [('i-ID-' + str(n)) + str(time.strftime(\"%m%d%Y\", time.localtime(time.time()))) for n in range(0, results)]\n",
    "    indeed_df = pd.DataFrame({'ID' : ID}).set_index('ID')\n",
    "    #################### search results ####################     \n",
    "    for i in range(tot_pg):\n",
    "        \n",
    "        ### Put together URL string and then get the page\n",
    "        url = 'https://www.indeed.com/jobs?q=' + job_title + '&l=' + city + '&start=' + str(i*10) + '&sort=' + 'date' + '&radius=' + '25'\n",
    "        print('Page {} url : {}'.format(i, url))\n",
    "        wd.get(url)\n",
    "        \n",
    "        ### Gets 10 IDs relevant to page\n",
    "        pg_IDs = ID[i*15:(i+1)*15]\n",
    "        \n",
    "        ### Length of page\n",
    "        listing_len = len(wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']\"))\n",
    "        \n",
    "        ### minimum between the (number of results on the page, number of results left to scrap)\n",
    "        result_length = min(listing_len, int(res_left)) \n",
    "        \n",
    "        ### Scrap different parts of page into `result_length` length lists and put them in the df\n",
    "        indeed_df.loc[pg_IDs, 'company_name'] = [company.text for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")][:result_length]\n",
    "        #indeed_df.loc[pg_IDs, 'job_title'] = [title.text for title in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']\")][:result_length]\n",
    "        indeed_df.loc[pg_IDs, 'location'] = [title.text for title in wd.find_elements_by_xpath(\"//span[@class='location']\")][:result_length]\n",
    "        indeed_df.loc[pg_IDs, 'description'] = [description.text for description in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='summary']\")][:result_length]\n",
    "        indeed_df.loc[pg_IDs, 'post_link'] = [post_link.get_attribute('href') for post_link in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']//a\")][:result_length]\n",
    "        indeed_df.loc[pg_IDs, 'comp_link'] = ['https://www.indeed.com/cmp/' + str(company.text).replace(' ', '-') for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")][:result_length]\n",
    "        post_links = [post_link.get_attribute('href') for post_link in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']//a\")][:result_length]\n",
    "        comp_links = ['https://www.indeed.com/cmp/' + str(company).replace(' ', '-') for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")][:result_length]\n",
    "        \n",
    "        ### Subtract length of this page of results from the total results left\n",
    "        res_left = res_left - result_length\n",
    "        \n",
    "        # Repeat loop through `tot_pg` pages then go to next section\n",
    "\n",
    "    #################### comp list ####################     \n",
    "    comp_list = []\n",
    "    comp_links = indeed_df['comp_link']\n",
    "    for index, link in enumerate(comp_links):\n",
    "        index = ID[index]\n",
    "        wd.get(link)\n",
    "        \n",
    "        ### If it can't find the company name or either link, just call the whole thing off\n",
    "        try:\n",
    "            indeed_df.loc[index, 'company_name'] = wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text\n",
    "            indeed_df.loc[index, 'salaries_link'] = 'https://www.indeed.com/cmp/' + wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text + '/salaries/'\n",
    "            indeed_df.loc[index, 'reviews_link'] = 'https://www.indeed.com/cmp/' + wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text + '/reviews/'\n",
    "            try:\n",
    "                indeed_df.loc[index, 'average_comp_rating'] = wd.find_element_by_xpath(\"//span[@class='cmp-header-rating-average']\").text\n",
    "            except:\n",
    "                indeed_df.loc[index, 'average_comp_rating'] = 'Not ranked.'\n",
    "        except:\n",
    "            indeed_df.loc[index, 'company_name'] = ''\n",
    "            indeed_df.loc[index, 'salaries_link'] = ''\n",
    "            indeed_df.loc[index, 'reviews_link'] = ''\n",
    "            continue\n",
    "\n",
    "    #################### Post list ####################     \n",
    "    post_list = []\n",
    "    post_links = indeed_df['post_link']\n",
    "    for index, link in enumerate(post_links):\n",
    "        index = ID[index]\n",
    "        wd.get(link)\n",
    "        line = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-InlineCompanyRating')]\").text\n",
    "        indeed_df.loc[index, 'company_location'] = line.split('\\n')[0] + ', ' + line.split('\\n')[-1]\n",
    "        \n",
    "        indeed_df.loc[index, 'job_title'] = wd.find_element_by_xpath(\"//h3[contains(@class, 'jobsearch-JobInfoHeader-title')]\").text\n",
    "        indeed_df.loc[index, 'post_body'] = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-JobComponent-description')]\").text\n",
    "        indeed_df.loc[index, 'post_time'] = wd.find_element_by_xpath(\"//div[@class='jobsearch-JobMetadataFooter']\").text.split(' - ')[1].strip()\n",
    "        \n",
    "    #################### Column Rename and Reorder ####################  \n",
    "    return indeed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ult_scrap(results, job_title, city, site_list, linkedin_username, linkedin_password, chromedriver_location, api_key, headless_arg, push_to_docs=False):\n",
    "\n",
    "    options = Options()\n",
    "    for arg in [ '--no-sandbox', '--disable-gpu', 'start-maximized', 'disable-infobars', '--disable-extensions']:\n",
    "        options.add_argument(arg)\n",
    "    if headless_arg == True:\n",
    "        options.add_argument('--headless')\n",
    "    wd = webdriver.Chrome(chromedriver_location, options=options)\n",
    "    \n",
    "    job_df = pd.DataFrame()\n",
    "    \n",
    "    for site in site_list: \n",
    "        \n",
    "        if site == 'stack':\n",
    "            stack_df = stack_scraper(wd, results, job_title, city)\n",
    "            job_df = job_df.append(stack_df, sort=False)\n",
    "            \n",
    "        if site == 'indeed':\n",
    "            indeed_df = indeed_scraper(wd, results, job_title, city)\n",
    "            job_df = job_df.append(indeed_df, sort=False)\n",
    "                    \n",
    "        if site == 'linkedin':\n",
    "            linkedin_df = linkedin_scraper(wd, results, job_title, city, linkedin_username, linkedin_password)\n",
    "            job_df = job_df.append(linkedin_df, sort=False)\n",
    "            \n",
    "    wd.stop_client()\n",
    "    wd.quit()\n",
    "    \n",
    "    if push_to_docs == True:\n",
    "        print(\"Pushing to docs .... \")\n",
    "        client = datasheets.Client()\n",
    "        workbook = client.fetch_workbook('Marketing Projections')\n",
    "        tab = workbook.fetch_tab('test')\n",
    "        tab.insert_data(job_df)\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indeed total pages: 1\n",
      "Page 0 url : https://www.indeed.com/jobs?q=data+scientist&l=boston&start=0&sort=date&radius=25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>post_link</th>\n",
       "      <th>comp_link</th>\n",
       "      <th>salaries_link</th>\n",
       "      <th>reviews_link</th>\n",
       "      <th>average_comp_rating</th>\n",
       "      <th>company_location</th>\n",
       "      <th>job_title</th>\n",
       "      <th>post_body</th>\n",
       "      <th>post_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i-ID-003272019</th>\n",
       "      <td>IBM</td>\n",
       "      <td>Cambridge, MA 02139 (Area IV area)</td>\n",
       "      <td>Watson Health is seeking an enthusiastic Cance...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=86f1585bc1c18...</td>\n",
       "      <td>https://www.indeed.com/cmp/IBM</td>\n",
       "      <td>https://www.indeed.com/cmp/IBM/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/IBM/reviews/</td>\n",
       "      <td>3.9</td>\n",
       "      <td>IBM, Cambridge, MA 02139</td>\n",
       "      <td>Watson Health Cancer Scientist Hematology</td>\n",
       "      <td>Job Description\\nThe IBM Watson Health busines...</td>\n",
       "      <td>1 hour ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-103272019</th>\n",
       "      <td>Siemens AG</td>\n",
       "      <td>Norwood, MA</td>\n",
       "      <td>Interface to R&amp;D scientists and R&amp;D functional...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=0e9bf6b32ee13...</td>\n",
       "      <td>https://www.indeed.com/cmp/Siemens</td>\n",
       "      <td>https://www.indeed.com/cmp/Siemens AG/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Siemens AG/reviews/</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Siemens, Norwood, MA</td>\n",
       "      <td>Commodity Management Specialist 4</td>\n",
       "      <td>Division: Siemens Healthineers\\nBusiness Unit:...</td>\n",
       "      <td>2 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-203272019</th>\n",
       "      <td>Staples</td>\n",
       "      <td>Framingham, MA 01702</td>\n",
       "      <td>While the emphasis of this position is hands-o...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=65ee74ef07380...</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples/reviews/</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Staples, Framingham, MA 01702</td>\n",
       "      <td>Principal Data Science Engineer</td>\n",
       "      <td>Description\\nDescription\\nWe are looking for a...</td>\n",
       "      <td>1 hour ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-303272019</th>\n",
       "      <td>Novartis</td>\n",
       "      <td>Cambridge, MA 02139 (Area IV area)</td>\n",
       "      <td>Good understanding of clinical study design pr...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d941d3d92b919...</td>\n",
       "      <td>https://www.indeed.com/cmp/Novartis</td>\n",
       "      <td>https://www.indeed.com/cmp/Novartis/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Novartis/reviews/</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Novartis, Cambridge, MA 02139</td>\n",
       "      <td>Principal Biostatistician - Biomarkers and Dia...</td>\n",
       "      <td>Principal Biostatistician - Biomarkers and Dia...</td>\n",
       "      <td>3 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-403272019</th>\n",
       "      <td>Amgen</td>\n",
       "      <td>Cambridge, MA 02139 (Area IV area)</td>\n",
       "      <td>The Amgen Postdoctoral program is committed to...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=54bbb676b390c...</td>\n",
       "      <td>https://www.indeed.com/cmp/Amgen</td>\n",
       "      <td>https://www.indeed.com/cmp/Amgen/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Amgen/reviews/</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Amgen, Cambridge, MA 02139</td>\n",
       "      <td>Postdoctoral Fellow – Computational/Systems Bi...</td>\n",
       "      <td>The Amgen Postdoctoral program is committed to...</td>\n",
       "      <td>1 hour ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-503272019</th>\n",
       "      <td>TechnipFMC</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Our Scientists take engineering designs and co...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=fde506c88873c...</td>\n",
       "      <td>https://www.indeed.com/cmp/TechnipFMC</td>\n",
       "      <td>https://www.indeed.com/cmp/TechnipFMC/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/TechnipFMC/reviews/</td>\n",
       "      <td>4.0</td>\n",
       "      <td>TechnipFMC, Boston, MA</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>nipFMC Process Technology jobs in Weymouth, MA...</td>\n",
       "      <td>9 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-603272019</th>\n",
       "      <td>Enanta Pharmaceuticals</td>\n",
       "      <td>Watertown, MA</td>\n",
       "      <td>A self-motivated and enthusiastic candidate wh...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=3f86b202a7cda...</td>\n",
       "      <td>https://www.indeed.com/cmp/Enanta-Pharmaceuticals</td>\n",
       "      <td>https://www.indeed.com/cmp/Enanta Pharmaceutic...</td>\n",
       "      <td>https://www.indeed.com/cmp/Enanta Pharmaceutic...</td>\n",
       "      <td>Not ranked.</td>\n",
       "      <td>Enanta Pharmaceuticals, Watertown, MA</td>\n",
       "      <td>Scientist, Formulation Development</td>\n",
       "      <td>Position Summary:\\n\\nA self-motivated and enth...</td>\n",
       "      <td>6 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i-ID-703272019</th>\n",
       "      <td>Mobiquity</td>\n",
       "      <td>Waltham, MA 02453</td>\n",
       "      <td>Web analytics, product analytics, conversion r...</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=7a44b07aa3be2...</td>\n",
       "      <td>https://www.indeed.com/cmp/Mobiquity</td>\n",
       "      <td>https://www.indeed.com/cmp/Mobiquity/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Mobiquity/reviews/</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Mobiquity, Waltham, MA 02453</td>\n",
       "      <td>Customer Experience Analytics Manager</td>\n",
       "      <td>Who We Are:\\nMobiquity is a digital engagement...</td>\n",
       "      <td>6 hours ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company_name                            location  \\\n",
       "ID                                                                           \n",
       "i-ID-003272019                     IBM  Cambridge, MA 02139 (Area IV area)   \n",
       "i-ID-103272019              Siemens AG                         Norwood, MA   \n",
       "i-ID-203272019                 Staples                Framingham, MA 01702   \n",
       "i-ID-303272019                Novartis  Cambridge, MA 02139 (Area IV area)   \n",
       "i-ID-403272019                   Amgen  Cambridge, MA 02139 (Area IV area)   \n",
       "i-ID-503272019              TechnipFMC                          Boston, MA   \n",
       "i-ID-603272019  Enanta Pharmaceuticals                       Watertown, MA   \n",
       "i-ID-703272019               Mobiquity                   Waltham, MA 02453   \n",
       "\n",
       "                                                      description  \\\n",
       "ID                                                                  \n",
       "i-ID-003272019  Watson Health is seeking an enthusiastic Cance...   \n",
       "i-ID-103272019  Interface to R&D scientists and R&D functional...   \n",
       "i-ID-203272019  While the emphasis of this position is hands-o...   \n",
       "i-ID-303272019  Good understanding of clinical study design pr...   \n",
       "i-ID-403272019  The Amgen Postdoctoral program is committed to...   \n",
       "i-ID-503272019  Our Scientists take engineering designs and co...   \n",
       "i-ID-603272019  A self-motivated and enthusiastic candidate wh...   \n",
       "i-ID-703272019  Web analytics, product analytics, conversion r...   \n",
       "\n",
       "                                                        post_link  \\\n",
       "ID                                                                  \n",
       "i-ID-003272019  https://www.indeed.com/rc/clk?jk=86f1585bc1c18...   \n",
       "i-ID-103272019  https://www.indeed.com/rc/clk?jk=0e9bf6b32ee13...   \n",
       "i-ID-203272019  https://www.indeed.com/rc/clk?jk=65ee74ef07380...   \n",
       "i-ID-303272019  https://www.indeed.com/rc/clk?jk=d941d3d92b919...   \n",
       "i-ID-403272019  https://www.indeed.com/rc/clk?jk=54bbb676b390c...   \n",
       "i-ID-503272019  https://www.indeed.com/rc/clk?jk=fde506c88873c...   \n",
       "i-ID-603272019  https://www.indeed.com/rc/clk?jk=3f86b202a7cda...   \n",
       "i-ID-703272019  https://www.indeed.com/rc/clk?jk=7a44b07aa3be2...   \n",
       "\n",
       "                                                        comp_link  \\\n",
       "ID                                                                  \n",
       "i-ID-003272019                     https://www.indeed.com/cmp/IBM   \n",
       "i-ID-103272019                 https://www.indeed.com/cmp/Siemens   \n",
       "i-ID-203272019                 https://www.indeed.com/cmp/Staples   \n",
       "i-ID-303272019                https://www.indeed.com/cmp/Novartis   \n",
       "i-ID-403272019                   https://www.indeed.com/cmp/Amgen   \n",
       "i-ID-503272019              https://www.indeed.com/cmp/TechnipFMC   \n",
       "i-ID-603272019  https://www.indeed.com/cmp/Enanta-Pharmaceuticals   \n",
       "i-ID-703272019               https://www.indeed.com/cmp/Mobiquity   \n",
       "\n",
       "                                                    salaries_link  \\\n",
       "ID                                                                  \n",
       "i-ID-003272019           https://www.indeed.com/cmp/IBM/salaries/   \n",
       "i-ID-103272019    https://www.indeed.com/cmp/Siemens AG/salaries/   \n",
       "i-ID-203272019       https://www.indeed.com/cmp/Staples/salaries/   \n",
       "i-ID-303272019      https://www.indeed.com/cmp/Novartis/salaries/   \n",
       "i-ID-403272019         https://www.indeed.com/cmp/Amgen/salaries/   \n",
       "i-ID-503272019    https://www.indeed.com/cmp/TechnipFMC/salaries/   \n",
       "i-ID-603272019  https://www.indeed.com/cmp/Enanta Pharmaceutic...   \n",
       "i-ID-703272019     https://www.indeed.com/cmp/Mobiquity/salaries/   \n",
       "\n",
       "                                                     reviews_link  \\\n",
       "ID                                                                  \n",
       "i-ID-003272019            https://www.indeed.com/cmp/IBM/reviews/   \n",
       "i-ID-103272019     https://www.indeed.com/cmp/Siemens AG/reviews/   \n",
       "i-ID-203272019        https://www.indeed.com/cmp/Staples/reviews/   \n",
       "i-ID-303272019       https://www.indeed.com/cmp/Novartis/reviews/   \n",
       "i-ID-403272019          https://www.indeed.com/cmp/Amgen/reviews/   \n",
       "i-ID-503272019     https://www.indeed.com/cmp/TechnipFMC/reviews/   \n",
       "i-ID-603272019  https://www.indeed.com/cmp/Enanta Pharmaceutic...   \n",
       "i-ID-703272019      https://www.indeed.com/cmp/Mobiquity/reviews/   \n",
       "\n",
       "               average_comp_rating                       company_location  \\\n",
       "ID                                                                          \n",
       "i-ID-003272019                 3.9               IBM, Cambridge, MA 02139   \n",
       "i-ID-103272019                 4.0                   Siemens, Norwood, MA   \n",
       "i-ID-203272019                 3.6          Staples, Framingham, MA 01702   \n",
       "i-ID-303272019                 4.1          Novartis, Cambridge, MA 02139   \n",
       "i-ID-403272019                 4.1             Amgen, Cambridge, MA 02139   \n",
       "i-ID-503272019                 4.0                 TechnipFMC, Boston, MA   \n",
       "i-ID-603272019         Not ranked.  Enanta Pharmaceuticals, Watertown, MA   \n",
       "i-ID-703272019                 3.6           Mobiquity, Waltham, MA 02453   \n",
       "\n",
       "                                                        job_title  \\\n",
       "ID                                                                  \n",
       "i-ID-003272019          Watson Health Cancer Scientist Hematology   \n",
       "i-ID-103272019                  Commodity Management Specialist 4   \n",
       "i-ID-203272019                    Principal Data Science Engineer   \n",
       "i-ID-303272019  Principal Biostatistician - Biomarkers and Dia...   \n",
       "i-ID-403272019  Postdoctoral Fellow – Computational/Systems Bi...   \n",
       "i-ID-503272019                                          Scientist   \n",
       "i-ID-603272019                 Scientist, Formulation Development   \n",
       "i-ID-703272019              Customer Experience Analytics Manager   \n",
       "\n",
       "                                                        post_body    post_time  \n",
       "ID                                                                              \n",
       "i-ID-003272019  Job Description\\nThe IBM Watson Health busines...   1 hour ago  \n",
       "i-ID-103272019  Division: Siemens Healthineers\\nBusiness Unit:...  2 hours ago  \n",
       "i-ID-203272019  Description\\nDescription\\nWe are looking for a...   1 hour ago  \n",
       "i-ID-303272019  Principal Biostatistician - Biomarkers and Dia...  3 hours ago  \n",
       "i-ID-403272019  The Amgen Postdoctoral program is committed to...   1 hour ago  \n",
       "i-ID-503272019  nipFMC Process Technology jobs in Weymouth, MA...  9 hours ago  \n",
       "i-ID-603272019  Position Summary:\\n\\nA self-motivated and enth...  6 hours ago  \n",
       "i-ID-703272019  Who We Are:\\nMobiquity is a digital engagement...  6 hours ago  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch = ult_scrap(**{\n",
    "    'results' : 8,\n",
    "    'job_title' : 'data+scientist',\n",
    "    'city' : 'boston',\n",
    "    'site_list' : ['indeed'],\n",
    "    'linkedin_username' : 'USERNAME',\n",
    "    'linkedin_password' : 'PASSWORD',\n",
    "    'chromedriver_location' : 'C:/Users/matt/Documents/STUFF/chromedriver/chromedriver.exe',\n",
    "    'api_key' : 'APIKEY' ,\n",
    "    'headless_arg' : True,\n",
    "    'push_to_docs' : False\n",
    "})\n",
    "fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument('--log-level=3')\n",
    "wd = webdriver.Chrome(scrap_params['chromedriver_location'], options=options)\n",
    "url = 'https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25'\n",
    "print(url)\n",
    "wd.get(url)\n",
    "companies = []\n",
    "\n",
    "#companies = [company.text for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']\")]\n",
    "companies = [company.text for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")]\n",
    "titles = [title.text for title in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']\")]\n",
    "description = [description.text for description in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='summary']\")]\n",
    "post_links = [post_link.get_attribute('href') for post_link in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']//a\")]\n",
    "comp_links = ['https://www.indeed.com/cmp/' + str(company).replace(' ', '-') for company in companies]\n",
    "\n",
    "tmp_df = pd.DataFrame({'ID' : 'ID', 'site' : 'indeed', 'title' : titles, 'company' : companies, 'summary' : description, 'post_links' : post_links, 'comp_links' : comp_links})\n",
    "\n",
    "comp_list = []\n",
    "for link in comp_links:\n",
    "    wd.get(link)\n",
    "    company_name = wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text\n",
    "    try:\n",
    "        average_comp_rating = wd.find_element_by_xpath(\"//span[@class='cmp-header-rating-average']\").text\n",
    "    except:\n",
    "        average_comp_rating = 'Not ranked.'\n",
    "    salaries_link = 'https://www.indeed.com/cmp/' + company_name + '/salaries/'\n",
    "    reviews_link = 'https://www.indeed.com/cmp/' + company_name + '/reviews/'\n",
    "    comp_list.append([company_name, average_comp_rating, salaries_link, reviews_link])\n",
    "comp_df = pd.DataFrame(comp_list, columns=['company', 'rating', 'salaries_link', 'reviews_link'])\n",
    "\n",
    "post_list = []\n",
    "for link in post_links:\n",
    "    wd.get(link)\n",
    "    jobtitle = wd.find_element_by_xpath(\"//h3[contains(@class, 'jobsearch-JobInfoHeader-title')]\").text\n",
    "    #jobtitle = wd.find_element_by_xpath(\"//h3[@class='jobsearch-JobInfoHeader-title']\").text\n",
    "    #company_name = wd.find_element_by_xpath(\"//div[@class='icl-u-lg-mr--sm icl-u-xs-mr--xs']\").text\n",
    "    try:\n",
    "        company_pic = wd.find_element_by_xpath(\"//img[@class='jobsearch-CompanyAvatar-image']\").get_attribute('src')\n",
    "    except:\n",
    "        company_pic = 'NA'\n",
    "    line = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-InlineCompanyRating')]\").text\n",
    "    location = line.split('\\n')[2]\n",
    "    company_name = line.split('\\n')[0]\n",
    "    body = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-JobComponent-description')]\").text\n",
    "    post_time = wd.find_element_by_xpath(\"//div[@class='jobsearch-JobMetadataFooter']\").text.split('-')[1].strip()\n",
    "    post_list.append([jobtitle, company_name, post_time, location, company_pic, body])\n",
    "post_df = pd.DataFrame(post_list, columns=['title', 'company', 'posttime', 'location', 'company_pic', 'body_text'])\n",
    "job_df = comp_df.merge(post_df, on='company', how='left')\n",
    "job_df = job_df.merge(tmp_df, on=['company', 'title'], how='left')\n",
    "wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df['comp_links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tex_output(fetch, suffix=''):\n",
    "    fetch_df = fetch\n",
    "    #fetch_dict = fetch[1]\n",
    "    map_list = []\n",
    "    content = r'''\\UseRawInputEncoding%\n",
    "\\documentclass{{article}}%\n",
    "\\usepackage[T1]{{fontenc}}%\n",
    "\\usepackage[utf8]{{inputenc}}%\n",
    "\\inputencoding{utf8}\n",
    "\\usepackage{{lmodern}}%\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{{textcomp}}%\n",
    "\\usepackage{{lastpage}}%\n",
    "\n",
    "\\begin{document}%\n",
    "\\normalsize%'''\n",
    "    \n",
    "    for index, row in fetch_df.iterrows():\n",
    "        print(index)\n",
    "        print(row['map_url'])\n",
    "        map_file_name = str(index) + '.jpg'\n",
    "        map_list.append(map_file_name)\n",
    "        urllib.request.urlretrieve(str(row['map_url']), map_file_name)\n",
    "        content = content + r'''\n",
    "\\section{{{jobtitle}}}%\n",
    "\\subsection{{{comploc}}}\n",
    "{{{address}}}\n",
    "{{{distance}}}\n",
    "{{{duration}}}\n",
    "\n",
    "\n",
    "\\begin{{figure}}\n",
    "\\includegraphics{{{map_file_name}}}\n",
    "\\end{{figure}}\n",
    "\n",
    "\\subsection{{Salary}}%\n",
    "{{{salary}}}\n",
    "\n",
    "\\subsection{{Link}}%\n",
    "{{{link}}}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\n",
    "'''.format(**{'jobtitle': row['title'], 'salary' : row['salary'], 'comploc' : row['comp_loc'], 'address' : row['address'], 'distance' : row['distance'], \n",
    "              'duration' : row['duration'], 'link' : row['link'], 'map_file_name' : map_file_name}).replace('&', '\\&').replace('$', '\\$').replace('#', '\\#').replace('|', '').replace('_', '\\_')        \n",
    "        filename = 'job_scrap_output' + suffix + '.tex'\n",
    "\n",
    "    content = content + '''\n",
    "\\end{document}%'''\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    cmd = ['pdflatex', filename]\n",
    "    subprocess.check_output(cmd)\n",
    "\n",
    "    os.unlink('job_scrap_output.log')\n",
    "    os.unlink('job_scrap_output.aux')\n",
    "    for file in map_list:\n",
    "        os.unlink(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tex_output(fetch, suffix=''):\n",
    "    fetch_df = fetch\n",
    "    #fetch_dict = fetch[1]\n",
    "    map_list = []\n",
    "    content = r'''\\documentclass[letterpaper,12pt,fleqn]{{article}}\n",
    "\\usepackage{{fancyhdr}}\n",
    "\\usepackage{{datetime}}\n",
    "\\usepackage{{hyperref}}\n",
    "\n",
    "% Insert Title Page Info Below\n",
    "\n",
    "\\newcommand{\\Titles}{Job ScrapyBoi} %\n",
    "\\newcommand{\\cityset}{Boston} %\n",
    "\\newcommand{\\jobset}{Data Science} %\n",
    "\\newcommand{\\authors}{Matt and Chris} %\n",
    "\\newcommand{\\sources}{Indeed, LinkedIn, StackOverflow} %\n",
    "\\newcommand{\\lastwords}{End of Results} %\n",
    "\n",
    "% Margins and Footer Style\n",
    "\\setlength{\\topmargin}{0cm} %\n",
    "\\setlength{\\textheight}{9.25in} %\n",
    "\\setlength{\\oddsidemargin}{0.0in}\n",
    "\\setlength{\\evensidemargin}{0.0in}\n",
    "\\setlength{\\textwidth}{16cm}\n",
    "\\pagestyle{fancy}\n",
    "\\lhead{{}} \n",
    "\\chead{{}} \n",
    "\\rhead{{}} \n",
    "\\lfoot{{}} \n",
    "\\cfoot{{\\footnotesize{{Page \\thepage \\ of \\pageref{{finalpage}}}}}} \n",
    "\\rfoot{{}} \n",
    "\n",
    "\\renewcommand{\\headrulewidth}{0pt} %Do not print a rule below the header\n",
    "\\renewcommand{\\footrulewidth}{0pt}\n",
    "\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "% Title page\n",
    "\n",
    "\\begin{center}\n",
    "\\vspace{10cm}\n",
    "\\huge\\textbf{\\Titles}\n",
    "\\end{center}\n",
    "\\vspace{4cm}\n",
    "\n",
    "\\begin{center}\n",
    "\\Large\\textbf{\\today}\n",
    "\\end{center}\n",
    "\n",
    "\n",
    "\\begin{center}\n",
    "\\Large\\textbf{\\cityset}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{center}\n",
    "\\large\\textbf{\\jobset}\n",
    "\\end{center}\n",
    "\\vspace{4cm}\n",
    "\n",
    "\\begin{center}\n",
    "\\textit{Authors: \\authors}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{center}\n",
    "\\textit{Sources: \\sources}\n",
    "\\end{center}\n",
    "\\newpage\n",
    "\n",
    "% End title page'''\n",
    "    \n",
    "    for index, row in fetch_df.iterrows():\n",
    "        print(index)\n",
    "        #print(row['map_url'])\n",
    "        #map_file_name = str(index) + '.jpg'\n",
    "        #map_list.append(map_file_name)\n",
    "        #urllib.request.urlretrieve(str(row['map_url']), map_file_name)\n",
    "        tmp_content = r'''\n",
    "\n",
    "\\bigskip\n",
    "\n",
    "\\paragraph {%(jobtitle)s} Company\n",
    "\n",
    "\\noindent\\rule{8cm}{0.4pt}\n",
    "\\begin{description}\n",
    "\\item \\lbrack \\ \\textit{Map} ] %(map)s\n",
    "\\item \\lbrack \\ \\textit{Distance} ] %(distance)s\n",
    "\\item \\lbrack \\ \\textit{Drive Duration} ] %(duration)s\n",
    "\\item \\lbrack \\ \\textit{Keywords} ] %(keywords)s\n",
    "\\item \\lbrack \\ \\textit{Salary} ] %(salary)s\n",
    "\\item \\lbrack \\ \\textit{Full Text} ] %(fulltext)s\n",
    "\\item \\lbrack \\ \\textit{Link} ] \\url{%(link)s}\n",
    "\\end{description}\n",
    "\n",
    "\\begin{center}\n",
    "\\vspace{3cm}\n",
    "--------- \\textit{\\lastwords} ---------\n",
    "\\end{center}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "'''\n",
    "        '''tmp_content = tmp_content.format(**)'''\n",
    "        tmp_content = tmp_content % { 'jobtitle' : row['title'], 'map' : row['map_url'] , 'distance' : row['distance'], 'duration' : 'other butts' , 'keywords' : 'butty' , 'salary' : 'moneybutts', \n",
    "                                            'fulltext' : row['body_text'], 'link' : row['post_link'] }\n",
    "        tmp_content = tmp_content.encode('ascii',errors='ignore').decode()\n",
    "        tmp_content = tmp_content.replace('&', '\\&').replace('$', '\\$').replace('#', '\\#').replace('|', '').replace('_', '\\_')\n",
    "        #tmp_content = codecs.encode(tmp_content, 'utf-8').decode('utf-8')\n",
    "        content = content + tmp_content\n",
    "        # ID \ttitle \tcompany \tlocation \tpost_link \tcomp_link \tezapply \tcompany_pic \tpulldate \tcomp_loc \trating \tsalaries_link \treviews_link \tl_company \tpostdate \tbody_text \tsummary \tbenefits\n",
    "        filename = 'job_scrap_output' + suffix + '.tex'\n",
    "\n",
    "    content = content + '''\\end{document}'''\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    cmd = ['pdflatex', filename]\n",
    "    subprocess.check_output(cmd)\n",
    "\n",
    "    os.unlink('job_scrap_output.log')\n",
    "    os.unlink('job_scrap_output.aux')\n",
    "    #for file in map_list:\n",
    "        #os.unlink(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_output(fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here's below\n",
    "\n",
    "'''indeed_pg_num = 0\n",
    "                site_ind = 0 # set to 0 at start of site and compared to results so there are 'results' number of pulls per site\n",
    "                list_o_divs = [] # holds divs if multiple pages\n",
    "                tmp_dict = {} # holds info on each div loop\n",
    "                # https://www.indeed.com/jobs?q=data+science&l=boston&sort=date\n",
    "                res_left = results\n",
    "                base = 'https://www.indeed.com/jobs?'\n",
    "                while res_left > 0:            \n",
    "                    params = {'q' : job_title,'l' : city, 'start' : (results-res_left),  'sort' : 'date', 'radius': '25'}\n",
    "                    page = requests.get(base, params) \n",
    "                    time.sleep(1)  \n",
    "                    soup = get_soup(page.text)\n",
    "                    divs = soup.find_all(name=\"div\", attrs={\"class\":\"row\"})\n",
    "                    list_o_divs.append(divs)\n",
    "                    res_left = res_left - len(divs)\n",
    "                    indeed_pg_num = indeed_pg_num + 1\n",
    "                print('total indeed pages : ' + str(indeed_pg_num))\n",
    "                for divs in list_o_divs:\n",
    "                    for div in divs:\n",
    "                        if (site_ind < results):\n",
    "                            tmp_dict['sID'] = 'ID' + str(num)\n",
    "                            tmp_dict['site'] = site\n",
    "                            tmp_dict['pulldate'] = str(time.strftime(\"%M%D%y\", time.localtime(time.time())))\n",
    "                            tmp_dict['link'] = 'http://www.indeed.com' + div.find(name='a', attrs={'class':'turnstileLink'})['href']\n",
    "                            #tmp_dict['link'] = extract_link(div, site)\n",
    "                            tmp_dict['title'] = div.find(name='a', attrs={'class':'turnstileLink'})['title']\n",
    "                            # tmp_dict['title'] = extract_job_title(div, site)\n",
    "                            tmp_dict['summary'] = div.find('span', attrs={'class': 'summary'}).text.strip()\n",
    "                            # tmp_dict['summary'] = extract_summary(div, site)\n",
    "                            #tmp_dict['salary'] = extract_salary(div, site)\n",
    "                            # tmp_dict['location'] = extract_location(div, site).strip() # two things\n",
    "                            if div.find('span', attrs={'class': 'location'}).text is None:\n",
    "                                tmp_dict['location'] = div.find('div', attrs={'class': 'location'}).text.strip()\n",
    "                            else:\n",
    "                                tmp_dict['location'] = div.find('span', attrs={'class': 'location'}).text.strip()\n",
    "                            tmp_dict['tags'] = extract_tags(div, site)\n",
    "                            #tmp_dict['id'] = extract_id(div, site)\n",
    "                            tmp_dict['id'] = div['id']\n",
    "                            if div.find('div', attrs={'class': 'iaP'}) is not None:\n",
    "                                tmp_dict['ezapply'] = True\n",
    "                            else:\n",
    "                                tmp_dict['ezapply'] = False\n",
    "                            # tmp_dict['ezapply'] = extract_easyapply(div, site)\n",
    "                            tmp_dict['postdate'] = extract_postdate(div, site)\n",
    "                            #tmp_dict['company'] = extract_company(div, site).strip()\n",
    "                            tmp_dict['company'] = div.find(name=\"span\", attrs={\"class\":\"company\"}).text.replace('\\n', '').strip()\n",
    "                            tmp_dict['comp_loc'] = str(extract_company(div, site).strip() + ', ' + extract_location(div, site).strip())\n",
    "                            tmp_df = pd.DataFrame(tmp_dict, index=[tmp_dict['sID']])\n",
    "                            job_df = job_df.append(tmp_df, sort=False)\n",
    "                            num = num + 1\n",
    "                            site_ind = site_ind + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''                site_ind = 0 # set to 0 at start of site and compared to results so there are 'results' number of pulls per site\n",
    "                list_o_divs = [] # holds divs if multiple pages\n",
    "                tmp_dict = {} # holds info on each div loop\n",
    "                # https://stackoverflow.com/jobs?q=data+scientist&l=boston&d=20&u=Miles\n",
    "                pg = 1 # page counter \n",
    "                base = 'https://stackoverflow.com/jobs?'\n",
    "                tot_pg = ((results // 20) + 1)\n",
    "                print('Stack overflow total pages: ' + str(tot_pg))\n",
    "                while pg <= tot_pg: \n",
    "                    params = {'q' : job_title, 'sort' : 'p', 'l' : city, 'd' : '20', 'u': 'Miles', 'pg': pg} \n",
    "                    page = requests.get(base, params) \n",
    "                    print(page.url)\n",
    "                    time.sleep(1)\n",
    "                    soup = get_soup(page.text)\n",
    "                    divs = soup.find_all(name=\"div\", attrs={\"class\":\"-job-summary\"})\n",
    "                    list_o_divs.append(divs)\n",
    "                    pg = pg + 1\n",
    "                for divs in list_o_divs:\n",
    "                    for div in divs:\n",
    "                        if (site_ind < results): # if there is a div past the number of results for the site, doesn't add it to tmp_dict\n",
    "                            tmp_dict['sID'] = 'ID' + str(num)\n",
    "                            tmp_dict['site'] = site\n",
    "                            tmp_dict['pulldate'] = str(time.strftime(\"%m-%d-%Y_%I-%M_%p\", time.localtime(time.time())))\n",
    "                            tmp_dict['link'] = 'https://www.stackoverflow.com' + div.find(name='a', attrs={'class':'s-link'})['href']\n",
    "                            # tmp_dict['link'] = extract_link(div, site)\n",
    "                            tmp_dict['title'] = div.find(name='a', attrs={'class':'s-link'})['title']\n",
    "                            # tmp_dict['title'] = extract_job_title(div, site)\n",
    "                            tmp_dict['summary'] = 'No Stack Summary yet'\n",
    "                            if div.find('div', attrs={'class': '-perks'}).text is None:\n",
    "                                tmp_dict['salary'] = 'NA'\n",
    "                            else:\n",
    "                                div.find('div', attrs={'class': '-perks'}).text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "                            # tmp_dict['salary'] = extract_salary(div, site)\n",
    "                            tmp_dict['location'] = div.find_all('div', attrs={'class': '-company'}).split('-')[1].text\n",
    "                            # tmp_dict['location'] = extract_location(div, site).strip()\n",
    "                            tmp_dict['tags'] = ', '.join(div.find_all('a', attrs={'class': 'post-tag job-link no-tag-menu'}))\n",
    "                            # tmp_dict['tags'] = extract_tags(div, site)\n",
    "                            tmp_dict['id'] = div.find('span', attrs={'class': 'fav-toggle'})['data-jobid']\n",
    "                            #tmp_dict['id'] = extract_id(div, site)\n",
    "                            tmp_dict['ezapply'] = 'No Easy Applications on Stack'\n",
    "                            if div.find('span', attrs={'class': 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t24'}).text is None:\n",
    "                                tmp_dict['postdate'] = div.find('span', attrs={'class': 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t32'}).text\n",
    "                            else:\n",
    "                                tmp_dict['postdate'] = div.find('span', attrs={'class': 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t24'}).text\n",
    "                            # tmp_dict['postdate'] = extract_postdate(div, site)\n",
    "                            tmp_dict['company'] = div.find_all(name=\"div\", attrs={\"class\":\"-company\"}).split('-')[0].text.replace('\\n', '').replace('\\r','')\n",
    "                            #tmp_dict['company'] = extract_company(div, site).split('-')[0].strip()\n",
    "                            tmp_dict['comp_loc'] = div.find_all('div', attrs={'class': '-company'}).text\n",
    "                            tmp_df = pd.DataFrame(tmp_dict, index=[tmp_dict['sID']])\n",
    "                            job_df = job_df.append(tmp_df, sort=False)\n",
    "                            num = num + 1\n",
    "                            site_ind = site_ind + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
